{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quelques exercices de Regression lineaires..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre objectif  est maintenant d'obtenir une expérience pratique et une intuition de la régression linéaire et de la régularisation. \n",
    "\n",
    "\n",
    "## Overview:\n",
    "\n",
    "Dans ce cahier, nous introduirons la notion de régression lineaire et verrons comment améliorer l'apprentissage et la prédiction en introduisant une régularisation. Nous nous concentrerons principalement sur les applications simples de la régression linéaire: minimiser l'erreur quadratique moyenne (MSE) sur les données d'apprentissage (c'est-à-dire l'erreur intra-échantillon) et voir comment nous nous en tirerons sur les données de test (c'est-à-dire les erreurs hors échantillon). ). \n",
    "\n",
    "Nous allons considérer le problème de la régression des moindres carrés ordinaire dans lequel la \"fonction d'erreur\" est définie comme le **carré de la déviation de notre prédicteur linéaire à la réponse vraie**. Nous allons cependant compléter cette fonction d'erreur avec un régularisateur qui évite les surajustements, pour obtenir la Régularisation de Ridge (ou de Tikhonov).\n",
    "\n",
    "Considérons des données de la forme $ (y_i, \\mathbf {x} ^ {(i)}) $ où l'index $ i = 1 \\ldots n $ dépasse le nombre d'exemples dans les données d'apprentissage et $ \\mathbf {x} ^ {(i)} $ est un vecteur de fonction dimensionnelle $ p $. Pour faciliter la notation, il est utile de définir la matrice $ X $ des datas qui est $ n \\times p $  dont les lignes, $ \\textbf {x} ^ {(1)}, \\cdots, \\textbf { x} ^ {(n)} $, sont les exemples et les colonnes, $ \\mathbf {X} _ {:, 1}, \\cdots, \\mathbf {X} _ {:, p} $, sont les \"caractéristiques mesurées \"(ie les prédicteurs).  Nous notons également le vecteur colonne dimensionnelle $ n $ de l'échantillon $ i $ as $ \\mathbf {y} _i $ et le vecteur colonne dimensionnel $ p $ des paramètres de régression $ \\mathbf {w} \\in \\mathbb {R} ^ p $.\n",
    "\n",
    "Pour la régression des moindres carrés ordinaire (pas de régularisation), nous minimisons la fonction de coût de perte au carré:\n",
    "$$\n",
    "\\underset{\\textbf{w}\\in\\mathbb{R}^p}{\\operatorname{min}} ||\\textbf{Xw}-\\textbf{y}||_2^2 = \\underset{\\textbf{w}\\in\\mathbb{R}^p}{\\operatorname{min}} \\,(\\mathbf{Xw}-\\mathbf{y})^T(\\mathbf{Xw}-\\mathbf{y}),\n",
    "$$\n",
    "ou de facon equivalente:\n",
    "$$\n",
    "\\underset{\\textbf{w}\\in\\mathbb{R}^p}{\\operatorname{min}}   \\sum_{i=1}^n (y_i -\\mathbf{w}\\cdot\\mathbf{x}^{(i)})^2.\n",
    "$$\n",
    "\n",
    "Si le rang de la matrice est tel que $rank(\\mathbf{X})=p$, namely, les prédicteurs (elements) de  $ \\mathbf {X} _ {:, 1}, \\cdots \\mathbf {X} _ {:, p} $ sont linéairement indépendants, et il existe alors une solution unique à ce problème:\n",
    "$$\n",
    "\\hat{\\textbf{w}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\textbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jouons d'abord avec les polynomes ##\n",
    "Nous allons commencer par essayer de fitter des données que nous allons générer à l'aide de polynomes. Commençons par créer nos données. En physique, nous avons souvent des signaux sinusoïdaux avec du bruit, c'est la situation que nous allons esssayer de reproduire.\n",
    "\n",
    "* Soit $(X,Y)$ une paire de variables aléatoires ayant les propriétés suivantes : $X$ suit une loi uniforme sur $[0,1]$ et $Y = sin(6X)+\\sigma Z$ où $\\sigma =0.5$ et $Z$ est une variable aléatoire indépendante de $X$ suivant une loi normale de paramètres (0,1).\n",
    "* Dans le modèle précédent, si par exemple nous mesurons un signal électrique au cours du temps, les réalisations de $X$ correspondent aux instants où la mesure est faite, $Y$ correspond au résultat de la mesure, et $Z$ au bruit possible. \n",
    "\n",
    "\n",
    "***Exercice :*** Générer 400 tirages de X et Y : $(x_i,y_i)$ pour $i=1....400$, puis les tracer avec sur le même graphe la fonction $f:x \\rightarrow sin(6x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Mettre votre code pour générer les données ici###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercice : *** Séparer les données en un ensemble de test(20%) et un ensemble d'entrainement(80%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Mettre votre code pour générer les données ici###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous allons essayer de fitter ces données précédentes avec des polynômes de différents degrés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit avec un polynôme de degré 1 ###\n",
    "Nous avons à notre disposition la formule du paragraphe précédent, la question est, comment mettre ce problème sous forme matricielle ? Il suffit en fait de construire une matrice X dont la dimension est $n\\times 2$ et où la ligne i est $(1, x_i)$\n",
    "\n",
    "\n",
    "*** Exercice :*** A l'aide de ce qui vient d'être dit, et de la formule du paragraphe précédent, écrire un programme qui résout le problème de régression avec un polynome de degré 1, et tracer le résultat sur une figure avec : les données, la fonction sin(6x), et le fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Mettre votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit avec des polynômes de degrés plus élevés ###\n",
    "Comme on pouvait s'en douter le fit avec un polynôme de degré 1 est de mauvaise qualité, nous allons donc essayer de fitter ces données avec des polynômes de degrés plus élevés :\n",
    "\n",
    "\n",
    "*** Exercice :*** Comment construire cette fois ci la matrice $X$ pour faire un fit avec un polynôme de degré arbitraire ? (indice dans l'exercice précédent on pouvait écire la ligne i de la forme $(x_i^0,x_i^1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercice :*** Coder un algorithme pour fitter les données à un polynôme de degré entre 1 et 15, et tracer les résultats, que remarque-t-on sur les degrés les plus élevés ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Mettre votre code ici####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercice :*** Pour confirmer votre intuition, tracer la RMSE sur le l'ensemble d'entrainement et de test en fonction du degré du polynôme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Mettre votre code ici###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment résoudre ce problème ? ###\n",
    "Souvent pour éviter l'overfit, on utilise une \"pénalité\" qui dépend des coefficients. Cette pénalité est soit la norme $L^2$ (ridge) soit la norme $L^1$ (LASSO), nous allons implémenter ces deux méthodes pour le modèle que nous avons étudié précédemment.\n",
    "#### Ridge regression ####\n",
    "Pour la regression ridge, le risque s'écrit :\n",
    "\n",
    "\n",
    "$$ R(w) = \\sum_{i=1}^{n} (wx_i -y_i)^2 + \\lambda \\sum_{i=1}^{n} w_i^2$$\n",
    "\n",
    "La solution du problème est alors (preuve en bonus à la fin) :\n",
    "$$ \\mathbf{w}_{ridge}= (\\mathbf{X}^T\\mathbf{X}+\\lambda I)^{-1}\\mathbf{X}^T \\textbf{y}$$\n",
    "\n",
    "***Exercice*** Coder un algorithme pour fitter les données à un polynôme de degré entre 1 et 15 avec ridge, et tracer les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Mettre votre code ici ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Numerical Experiments with Ridge Regression and LASSO##\n",
    "\n",
    "Nous allons maintenant effectuer quelques expériences numériques avec la base de données sur le diabète en essayant de prédire l’évolution du diabète un an plus tard. Pour plus d'informations sur cet ensemble de données, voir <a href=\"https://archive.ics.uci.edu/ml/datasets/Diabetes\"> https://archive.ics.uci.edu/ml/datasets/ Diabète </a>. Cet ensemble de données a été décrit dans le célèbre <a href=\"http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf\"> régression par le moindre angle </a> par Efron, Hastie, Johnstone, Tibshirani. comme suit:\n",
    "<blockquote> Dix variables de base, l'âge, le sexe, l'indice de masse corporelle, la pression artérielle moyenne et six mesures de sérum sanguin ont été obtenus pour chacun des patients diabétiques à n $ = 442 $, ainsi que\n",
    "réponse d'intérêt, une mesure quantitative de la progression de la maladie un an après le début de l'étude. \n",
    "Nous commençons par tracer les poids pour chaque valeur de $ \\lambda $ pour Ridge Regression et LASSO. Cela s'appelle un chemin de régularisation. Nous comparons également les performances intra-échantillon et non échantillonné entre deux régressions en examinant le coefficient de détermination $ R ^ 2 $ (pour une définition détaillée, voir <a href = \"https://en.wikipedia.org/wiki/ Coefficient_de_détermination \"> ici </a>). En termes de régression linéaire, $ R ^ 2 $ nous indique dans quelle mesure la fonction de régression correspond aux données. Le meilleur ajustement possible correspond à $ R ^ 2 = 1 $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ***Exercice :*** Compléter le code suivant en utilisant la bibliothèque sklearn : placer pour chaque valeur de $\\lambda$ (dans le tableau lambdas) les valeurs des coefficients calculés par méthode ridge et LASSO dans les tableaux coefs_ridge et coefs_lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-925c825b334c>, line 55)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-925c825b334c>\"\u001b[0;36m, line \u001b[0;32m55\u001b[0m\n\u001b[0;31m    plt.subplot(1,2,1)\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "print(__doc__)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# This code is modified from plot_cv_diabetes.py in the skit-learn documentation\n",
    "# and plot_ridge_path.py\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "# Load Training Data set with 200 examples\n",
    "\n",
    "number_examples=200\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data[:number_examples]\n",
    "y = diabetes.target[:number_examples]\n",
    "\n",
    "# Set up Lasso and Ridge Regression models\n",
    "\n",
    "\n",
    "# Chooose paths\n",
    "lambdas = np.logspace(-2, 2, 10)\n",
    "\n",
    "# To see how well we learn, we partition the dataset into a training set with 150 \n",
    "# as well as a test set with 50 examples. We record their errors respectively.\n",
    "\n",
    "n_samples = 150\n",
    "n_samples_train = 100\n",
    "X_train, X_test = X[:n_samples_train], X[n_samples_train:]\n",
    "y_train, y_test = y[:n_samples_train], y[n_samples_train:]\n",
    "train_errors_ridge = list()\n",
    "test_errors_ridge = list()\n",
    "\n",
    "train_errors_lasso = list()\n",
    "test_errors_lasso = list()\n",
    "\n",
    "# Initialize coeffficients for ridge regression and Lasso\n",
    "\n",
    "coefs_ridge = []\n",
    "coefs_lasso=[]\n",
    "for a in lambdas:\n",
    "###Code à insérer ici, les fonctions à trouver sont dans sklearn\n",
    "    \n",
    "###############################################################################\n",
    "# Display results\n",
    "\n",
    "# First see how the 10 features we learned scale as we change the regularization parameter\n",
    "plt.subplot(1,2,1)\n",
    "plt.semilogx(alphas, np.abs(coefs_ridge))\n",
    "axes = plt.gca()\n",
    "#ax.set_xscale('log')\n",
    "#ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.xlabel(r'$\\lambda$',fontsize=18)\n",
    "plt.ylabel('$|w_i|$',fontsize=18)\n",
    "plt.title('Ridge')\n",
    "#plt.savefig(\"Ridge_sparsity_scale.pdf.pdf\")\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.semilogx(alphas, np.abs(coefs_lasso))\n",
    "axes = plt.gca()\n",
    "#ax.set_xscale('log')\n",
    "#ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.xlabel(r'$\\lambda$',fontsize=18)\n",
    "#plt.ylabel('$|\\mathbf{w}|$',fontsize=18)\n",
    "plt.title('LASSO')\n",
    "#plt.savefig(\"LASSO_sparsity_scale.pdf\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot our performance on both the training and test data\n",
    "plt.semilogx(alphas, train_errors_ridge, 'b',label='Train (Ridge)')\n",
    "plt.semilogx(alphas, test_errors_ridge, '--b',label='Test (Ridge)')\n",
    "plt.semilogx(alphas, train_errors_lasso, 'g',label='Train (LASSO)')\n",
    "plt.semilogx(alphas, test_errors_lasso, '--g',label='Test (LASSO)')\n",
    "#plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n",
    "#           linewidth=3, label='Optimum on test')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim([-0.01, 1.0])\n",
    "plt.xlabel(r'$\\lambda$',fontsize=18)\n",
    "plt.ylabel('Performance')\n",
    "#plt.savefig(\"Ridge_LASSO_sparsity_performance.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTIONS: ###  \n",
    "<ul>\n",
    "<li> À quoi correspondent les points $ \\lambda = 0 $ et $ \\lambda = 10 ^ 5 $? Est-ce étrange que les poids ne soient pas monotones dans $ \\lambda $? Pourquoi pensez-vous que cela pourrait être?\n",
    "<li> Quelle est la différence qualitative entre le chemin LASSO et le chemin Ridge? Est-ce que cela concorde avec vos prédictions antérieures? Pouvez-vous présenter un argument qualitatif pour rationaliser cette différence?\n",
    "<li> En quoi vos réponses changent-elles lorsque vous modifiez le nombre d'exemples et la taille de l'ensemble de formation?\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le reseau elastique (Elastic-net)\n",
    "\n",
    "La méthode Elastic-Net combine les atouts des méthodes Ridge et LASSO:\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{w}\\in\\mathbb{R}^p}{\\operatorname{min}} ||\\mathbf{Xw}-\\mathbf{y}||_2^2 + \\lambda ||\\mathbf{w}||_1 + \\delta||\\mathbf{w}||_2^2,\n",
    "$$\n",
    "où $ \\lambda, \\delta  $ sont des paramètres de régularisation. Désormais, mis à part le caractère unique de la solution, le réseau-élastique combine certaines des propriétés souhaitables (par exemple, la prédiction) de la régression de Ridge avec les propriétés de parcimonie du LASSO:\n",
    "\n",
    "### Exercise : ###  \n",
    "<ul>\n",
    "<li> Jouez avec les paramètres $ \\lambda $ et $ \\delta $, quand vous attendriez-vous à des solutions rares?\n",
    "\n",
    "<li> Tracez le chemin de régularisation du filet élastique. Comment cela dépend-il de $ \\lambda $ et $ \\delta $ (tracez un graphe de l'erreur en fonction de $\\lambda$ et $\\delta$)?\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Mettre votre code ici####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BONUS : LASSO regression ####\n",
    "Pour LASSO, la tâche est un petit peu plus compliquée en effet, comme la dérivée du risque est un petit peu plus compliquée, il n'existe pas de explicite pour trouver les coefficients, nous allons procéder ensemble. L'algorithme s'appelle la descente de coordonnées. Nous avons la formule suivante :\n",
    "$$w^i_{LASSO} = \\dfrac  {S_{\\lambda} ( X_i^T (y- X_{-i}w_{-i}))}{X_i^TX_i}$$\n",
    "$X_i$ correspond à la colonne i de la matrice X, et $X_{-i}$ correspond à la matrice X à laquelle on a enlevée la colonne i, $w_{-i}$ correspond au vecteur i auquel on a enlevé l'élément i. Enfin $S_{\\lambda}$ est une fonction appelée \"soft threshold function\" définie comme ceci :\n",
    "$S_{\\lambda}(x) = 0$ si $|x|<\\lambda$ et $S_{\\lambda}(x) = x-\\lambda sign(x)$ sinon\n",
    "\n",
    "*** Exercice *** Concevoir un algorithme, et tracer les résultats (indice : avec la formule précédente on a l'expression d'une coordonnée en fonction de toutes les autres, on peut donc procéder avec plusieurs itérations jusqu'à convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Mettre votre code ici ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions théoriques bonus ###  \n",
    "<ul>\n",
    "\n",
    "<li> Dérivez $ \\hat {\\textbf {w}} $ explicitement en résolvant le problème des moindres carrés défini ci-dessus.\n",
    "\n",
    "\n",
    "<li> Est-ce que $ \\hat {\\textbf {w}} $ est toujours bien défini quand le rang $r(\\mathbf {X})<p$? Cela se produit lorsque, par exemple, $ n<p $.\n",
    "\n",
    "<li> Imaginons maintenant que les exemples soient générés de la manière suivante: $ y_i = \\textbf {w} _ \\text {true} \\cdot \\textbf {x} ^ {(i)} + \\epsilon_i $ où les $ \\epsilon_i \\sim \\mathcal {N} (0, \\sigma ^ 2) $ sont iid Gaussiens. Le risque intra-échantillon est défini comme\n",
    "$$\n",
    "R (\\hat {\\textbf {w}}, \\textbf {w} _ \\text {true}) = \\frac {1} {n} \\mathbb {E} [(\\mathbf {X} \\hat {\\textbf {w}} - \\mathbf {X} {\\textbf {w} _ \\text {true}}) ^ 2],\n",
    "$$\n",
    "où $ \\mathbb {E} [\\cdots] $ est repris sur toutes les paires iid $ (y_i, \\textbf {x} ^ {(i)}) $ et $ \\hat {\\textbf {w}} $ est le moins carrés solution donnée ci-dessus. En supposant que $ \\mathbf {X} $ et $ \\epsilon_i $ soient indépendants, montrez que le risque est donné par\n",
    "\n",
    "$$\n",
    "R (\\hat {\\textbf {w}}, \\textbf {w} _ \\text {true}) = \\sigma ^ 2 \\frac {p} {n}\n",
    "$$\n",
    "Quelle est l’implication de cela pour $ p $ fixe en tant que $ n \\rightarrow \\infty $? Qu'en est-il de quand $ p, n $ évolue ensemble?\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises pour RIDGE: ##\n",
    "\n",
    "Dans la Ridge-Regression, la pénalité de régularisation est la norme L2 des paramètres\n",
    "$$\n",
    "\\mathbf{w}_{ridge}(\\lambda)= \\underset{\\textbf{w}\\in\\mathbb{R}^p}{\\operatorname{argmin}} ||\\mathbf{X}\\textbf{w}-\\textbf{y}||_2^2 + \\lambda ||\\textbf{w}||_2^2.\n",
    "$$\n",
    "\n",
    "Ainsi, le modèle est ajusté en minimisant la somme de l'erreur dans l'échantillon et du terme de régularisation\n",
    "$$\n",
    "\\mathbf{w}_{ridge}(\\lambda)= \\underset{\\textbf{w}\\in\\mathbb{R}^p}{\\operatorname{argmin}} ||\\mathbf{X}\\textbf{w}-\\textbf{y}||_2^2 + \\lambda ||\\textbf{w}||_2^2.\n",
    "$$\n",
    "Notez que le paramètre $ \\lambda $ contrôle combien nous pesons le terme d’ajustement et de régularisation.\n",
    "\n",
    "<ul>\n",
    "<li>Montrer que la solution est donnée par $\\mathbf{w}_{ridge}= (\\mathbf{X}^T\\mathbf{X}+\\lambda I)^{-1}\\mathbf{X}^T \\textbf{y}$. \n",
    "<li>Exprimez votre réponse en termes de décomposition en valeurs singulières de $\\mathbf{X}$.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises pour LASSO ##\n",
    "\n",
    "Nous nous intéresserons également au cas où la pénalité est la norme L1 des paramètres (somme des valeurs absolues des paramètres). Ceci s'appelle LASSO.\n",
    "$$\n",
    "E_{LASSO}= \\lambda ||\\mathbf{w}||_1 = \\lambda \\sum_{\\gamma=1}^p |w_\\gamma| .\n",
    "$$\n",
    "Dans ce cas,\n",
    "$$\n",
    "\\textbf{w}_{LASSO}(\\lambda)= \\underset{\\textbf{w}\\in\\mathbb{R}^p}{\\operatorname{argmin}} {1 \\over 2n} ||\\mathbf{Xw}-\\mathbf{y}||_2^2 + \\lambda ||\\mathbf{w}||_1.\n",
    "$$\n",
    "Notez que le préfacteur $ 1 / (2n) $ dans la fonction de perte n'est pas essentiel à cette formulation. Nous avons choisi cette forme pour être cohérente avec le package Scikit-Learn en Python. Comme nous en avons discuté en cours, LASSO a tendance à donner des solution parcimonieuses. \n",
    "\n",
    "<ul>\n",
    "<li> Pouvez-vous dériver une expression analytique pour $ \\mathbf {w} _ {LASSO} $? Avez-vous des idées sur la manière dont nous pourrions être en mesure de le calculer numériquement efficacement?\n",
    "<li> Pensez-vous que LASSO et Ridge Regression donneront des réponses qualitativement différentes? (Considérez les limites $ \\lambda = 0$ et $ \\lambda = \\infty $)\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
